{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import when, col, regexp_replace"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "23/09/05 13:24:37 WARN Utils: Your hostname, ubuntu-Lenovo-Legion-5-15ARH05 resolves to a loopback address: 127.0.1.1; using 172.16.5.112 instead (on interface wlp4s0)\n",
      "23/09/05 13:24:37 WARN Utils: Set SPARK_LOCAL_IP if you need to bind to another address\n",
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n",
      "23/09/05 13:24:38 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n"
     ]
    }
   ],
   "source": [
    "# Initialize a Spark session\n",
    "spark = SparkSession.builder.appName(\"DataTransformation\").getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "# Load the CSV data into DataFrames\n",
    "listing_df_raw = spark.read.csv('raw_data/listings.tsv', header=True, inferSchema=True, sep=\"\\t\")\n",
    "reviews_df_raw = spark.read.csv('raw_data/reviews.tsv', header=True, inferSchema=True, sep=\"\\t\")\n",
    "calendar_df_raw = spark.read.csv('raw_data/calendar.tsv', header=True, inferSchema=True,sep=\"\\t\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+----------+---------+-----+\n",
      "|listing_id|date      |available|price|\n",
      "+----------+----------+---------+-----+\n",
      "|12147973  |2017-09-05|f        |null |\n",
      "|12147973  |2017-09-04|f        |null |\n",
      "|12147973  |2017-09-03|f        |null |\n",
      "|12147973  |2017-09-02|f        |null |\n",
      "|12147973  |2017-09-01|f        |null |\n",
      "|12147973  |2017-08-31|f        |null |\n",
      "|12147973  |2017-08-30|f        |null |\n",
      "|12147973  |2017-08-29|f        |null |\n",
      "|12147973  |2017-08-28|f        |null |\n",
      "|12147973  |2017-08-27|f        |null |\n",
      "|12147973  |2017-08-26|f        |null |\n",
      "|12147973  |2017-08-25|f        |null |\n",
      "|12147973  |2017-08-24|f        |null |\n",
      "|12147973  |2017-08-23|f        |null |\n",
      "|12147973  |2017-08-22|f        |null |\n",
      "|12147973  |2017-08-21|f        |null |\n",
      "|12147973  |2017-08-20|f        |null |\n",
      "|12147973  |2017-08-19|f        |null |\n",
      "|12147973  |2017-08-18|f        |null |\n",
      "|12147973  |2017-08-17|f        |null |\n",
      "+----------+----------+---------+-----+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "calendar_df_raw.show(truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Transform the data\n",
    "\n",
    "# LISTINGS DATA\n",
    "\n",
    "# Drop the 'summary' column\n",
    "listing_df_raw = listing_df_raw.drop('summary')\n",
    "listing_df_raw = listing_df_raw.drop('description')\n",
    "listing_df_raw = listing_df_raw.drop('host_about')\n",
    "\n",
    "\n",
    "# Convert 'host_is_superhost' to boolean\n",
    "listing_df_raw = listing_df_raw.withColumn('host_is_superhost', when(col('host_is_superhost') == 't', True).otherwise(False))\n",
    "\n",
    "# Drop 'country' and 'market' columns\n",
    "listing_df_raw = listing_df_raw.drop('country', 'market')\n",
    "\n",
    "# Drop rows with null values in the 'space' column\n",
    "listing_df_raw = listing_df_raw.na.drop(subset=['space'])\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Replace commas in all columns using a loop\n",
    "for column in listing_df_raw.columns:\n",
    "    listing_df_raw = listing_df_raw.withColumn(column, regexp_replace(col(column), ',', ''))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "# CALENDAR DATA\n",
    "\n",
    "# Fill null values in 'price' column with \"Booked\"\n",
    "calendar_df_raw = calendar_df_raw.na.fill({'price': 'Booked'})\n",
    "\n",
    "# Convert 'available' to boolean\n",
    "calendar_df_raw = calendar_df_raw.withColumn('available', when(col('available') == 't', True).otherwise(False))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "# REVIEW DATA\n",
    "\n",
    "# No transformation needed\n",
    "\n",
    "# SAVE DATA\n",
    "\n",
    "listing_df_raw.coalesce(1).write.csv('cleaned_data/clean_listing_csv', header=True, mode=\"overwrite\")\n",
    "calendar_df_raw.coalesce(1).write.csv('cleaned_data/clean0_calendar_csv', header=True, mode=\"overwrite\")\n",
    "reviews_df_raw.coalesce(1).write.csv('cleaned_data/clean_reviews_csv', header=True, mode=\"overwrite\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+\n",
      "|                name|\n",
      "+--------------------+\n",
      "|Sunny Bungalow in...|\n",
      "|Charming room in ...|\n",
      "|Mexican Folk Art ...|\n",
      "|Spacious Sunny Be...|\n",
      "| Come Home to Boston|\n",
      "|Private Bedroom +...|\n",
      "|New Lrg Studio ap...|\n",
      "|\"Tranquility\" on ...|\n",
      "|6 miles away from...|\n",
      "|Perfect & Practic...|\n",
      "|Quiet  Beauty in ...|\n",
      "|Cozy Room & Fresh...|\n",
      "|Convient Safe and...|\n",
      "|Cozy room in a ch...|\n",
      "|Arborside Guest C...|\n",
      "|Skyline View to B...|\n",
      "|Spacious 3 bedroo...|\n",
      "|4BD/3.5BA Perfect...|\n",
      "|Private room in h...|\n",
      "|Surround yourself...|\n",
      "+--------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Use the read method to read the CSV file into a DataFrame\n",
    "read_df = spark.read.csv('cleaned_data/clean_listing_csv/', header=True, inferSchema=True)\n",
    "read_df.select(col('name')).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
